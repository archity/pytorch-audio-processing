{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "urbansound_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPtcFm9d9XtT"
      },
      "source": [
        "# Training a Sound Classifier with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3-IKHaoCMU7"
      },
      "source": [
        "## 0. Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwb7nJId-O4H",
        "outputId": "9262fbf4-354d-454f-d655-17e046dfd38a"
      },
      "source": [
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.1 torchsummary==1.5.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1MB 21kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4MB 116kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.8.1 torchaudio-0.8.1 torchvision-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcJrvbm99dXn"
      },
      "source": [
        "## 1. CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4XMcgxz9ykD"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 4 conv blocks -> flatten -> Linear -> SoftMax\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(in_features=128 * 5 * 4, out_features=10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear(x)\n",
        "        predictions = self.softmax(logits)\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHJSlrX99gaN"
      },
      "source": [
        "## 2. UrbanSoundDataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDe0z8tl97jf"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "\n",
        "class UrbanSoundDataset(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.device = device\n",
        "        self.transformation = transformation.to(self.device)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get the audio sample at 'index'\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "\n",
        "        # Get the label associated with this audio sample path\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "\n",
        "        # Register the signal to the device\n",
        "        signal = signal.to(self.device)\n",
        "\n",
        "        # Make sure that sample rate is same for all\n",
        "        signal = self._resample_if_necessary(signal, sr)\n",
        "\n",
        "        # Use a single channel (mono) in case the audio has multi-channels\n",
        "        signal = self._mix_down_if_necessary(signal)\n",
        "\n",
        "        # In case our audio file has more samples than the ones we need (num_sammples)\n",
        "        signal = self._cut_if_necessary(signal)\n",
        "\n",
        "        # In case our audio file has less samples than the ones we need (num_sammples)\n",
        "        signal = self._right_pad_if_necessary(signal)\n",
        "\n",
        "        signal = self.transformation(signal)\n",
        "        return signal, label\n",
        "\n",
        "    def _cut_if_necessary(self, signal):\n",
        "        # signal -> Tensor -> (num_channels, num_samples) -> (1, num_samples) -> (1, 50000) -> (1, 22050)\n",
        "        if signal.shape[1] > self.num_samples:\n",
        "            signal = signal[:, :self.num_samples]\n",
        "        return signal\n",
        "\n",
        "    def _right_pad_if_necessary(self, signal):\n",
        "        # [1, 1, 1] -> [1, 1, 1, 0, 0]\n",
        "        length_signal = signal.shape[1]\n",
        "        if length_signal < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - length_signal\n",
        "\n",
        "            # We want to do right-padding (append and NOT pre-pend)\n",
        "            # Example [1, 1, 1] -> [1, 1, 1, 0, 0]\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "        return signal\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal):\n",
        "        # signal -> (num_channels, samples) -> (2, 16000) -> (1, 16000)\n",
        "\n",
        "        # If audio is not mono (more than 1 channel)\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        # Get the fold number in format \"foldx\", where x is the fold number\n",
        "        # present in the 6th coloumn of the csv file.\n",
        "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
        "\n",
        "        # Get the complete path of the audio file\n",
        "        # audio_dir/fold/{name of audio file}\n",
        "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
        "        return path\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        # Get class label (7th coloumn in the CSV file)\n",
        "        return self.annotations.iloc[index, 6]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MevYOcxDAgt"
      },
      "source": [
        "## 3. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAbayfGfC_5q",
        "outputId": "c028a001-55d4-49b0-ce0c-a9d211bb4cc5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufk4K4kf9pyJ"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Nz0DNG9P20",
        "outputId": "7771c2fb-b6b7-4cf9-9c22-bb741c6c1062"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "ANNOTATIONS_FILE = \"/content/gdrive/MyDrive/Datasets/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
        "AUDIO_DIR = \"/content/gdrive/MyDrive/Datasets/UrbanSound8K/audio\"\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "\n",
        "\n",
        "def train_one_epoch(model, data_loader, loss_func, optimiser, device):\n",
        "    for input, target in data_loader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        # Calculate loss\n",
        "        predictions = model(input)\n",
        "        loss = loss_func(predictions, target)\n",
        "\n",
        "        # Reset gradients to zero after every batch of iteration\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # Backpropogate loss and update weights\n",
        "        loss.backward()  # Backpropogate\n",
        "        optimiser.step()  # Update the weights\n",
        "\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_func, optimiser, device, epochs):\n",
        "    \"\"\"\n",
        "    Function that trains over all the epochs, one by one.\n",
        "\n",
        "    :param model: The feed-forward model class object\n",
        "    :param data_loader: Pytorch's DataLoader class object, with defined batch size for loading\n",
        "    :param loss_func: Function for evaluating th loss\n",
        "    :param optimiser: Adam optimizer, with learning rate given as LEARNING_RATE\n",
        "    :param device: CPU/GPU\n",
        "    :param epochs: The number of EPOCHS defined\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i + 1}\")\n",
        "        train_one_epoch(model, data_loader, loss_func, optimiser, device)\n",
        "        print(\"----------------\")\n",
        "    print(\"Training finished\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check for GPU availability\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "\n",
        "    # Instantiate our dataset object\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        n_mels=64\n",
        "    )\n",
        "\n",
        "    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)\n",
        "\n",
        "    # Create data loader for train set\n",
        "    train_data_loader = DataLoader(usd, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Build model\n",
        "    cnn = CNNNetwork().to(device)\n",
        "    summary(model=cnn, input_size=(1, 64, 44))\n",
        "\n",
        "    # Instantiate loss func + optimiser\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    train(model=cnn,\n",
        "          data_loader=train_data_loader,\n",
        "          loss_func=loss_func,\n",
        "          optimiser=optimiser,\n",
        "          device=device, epochs=EPOCHS)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n",
        "    print(\"Model trained and saved to feedforwardnet.pth\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 66, 46]             160\n",
            "              ReLU-2           [-1, 16, 66, 46]               0\n",
            "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
            "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
            "              ReLU-5           [-1, 32, 35, 25]               0\n",
            "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
            "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
            "              ReLU-8           [-1, 64, 19, 14]               0\n",
            "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
            "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
            "             ReLU-11           [-1, 128, 11, 9]               0\n",
            "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
            "          Flatten-13                 [-1, 2560]               0\n",
            "           Linear-14                   [-1, 10]          25,610\n",
            "          Softmax-15                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 122,762\n",
            "Trainable params: 122,762\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.83\n",
            "Params size (MB): 0.47\n",
            "Estimated Total Size (MB): 2.31\n",
            "----------------------------------------------------------------\n",
            "Epoch 1\n",
            "Loss: 2.3068976402282715\n",
            "----------------\n",
            "Epoch 2\n",
            "Loss: 2.2681915760040283\n",
            "----------------\n",
            "Epoch 3\n",
            "Loss: 2.0347445011138916\n",
            "----------------\n",
            "Epoch 4\n",
            "Loss: 2.0649375915527344\n",
            "----------------\n",
            "Epoch 5\n",
            "Loss: 2.0274853706359863\n",
            "----------------\n",
            "Epoch 6\n",
            "Loss: 2.0636909008026123\n",
            "----------------\n",
            "Epoch 7\n",
            "Loss: 2.0650107860565186\n",
            "----------------\n",
            "Epoch 8\n",
            "Loss: 1.9864895343780518\n",
            "----------------\n",
            "Epoch 9\n",
            "Loss: 1.9869014024734497\n",
            "----------------\n",
            "Epoch 10\n",
            "Loss: 1.9901615381240845\n",
            "----------------\n",
            "Training finished\n",
            "Model trained and saved to feedforwardnet.pth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}